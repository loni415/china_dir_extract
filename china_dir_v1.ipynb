{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f2cbf1",
   "metadata": {},
   "source": [
    "in terminal:\n",
    "conda init\n",
    "conda activate china_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U PyMuPDF\n",
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b891c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d267b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lukasfiller/dev/china_directory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c756caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- START OF CONFIGURATION ---\n",
    "# Change these values for your specific run\n",
    "PDF_PATH = \"/Users/lukasfiller/dev/china_directory/China Directory 2024-2p5-50.pdf\"\n",
    "OUTPUT_PATH = \"output.json\"\n",
    "MODEL_NAME = \"llama3.3:70b-instruct-q4_K_M\"\n",
    "OLLAMA_HOST = \"http://127.0.0.1:11434\"\n",
    "OLLAMA_CLIENT_TIMEOUT = 300  # Seconds\n",
    "DEBUG_PRINT_JSON = True\n",
    "\n",
    "# --- NEW: Set a limit for testing ---\n",
    "# Set to a number (e.g., 5) for a sample run, or None to process the whole document.\n",
    "MAX_PAGES_TO_PROCESS = 5 \n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# This is the instruction set we developed, telling the LLM its role and rules.\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert data extraction system. Your sole function is to parse the provided text from a single page of a Chinese party-state leadership directory and convert its contents into a valid JSON array. Adhere strictly to the schema and parsing rules. Produce only the final JSON array as your output, without any commentary, apologies, or markdown code fences.\n",
    "\n",
    "The input text is from a two-column document. I have pre-processed it by merging lines from the left and right columns. A \"||\" separator often indicates the split between the columns.\n",
    "\n",
    "**Required Output JSON Schema:**\n",
    "[\n",
    "  {\n",
    "    \"organization_name_english\": \"string\",\n",
    "    \"organization_name_chinese\": \"string\",\n",
    "    \"document_section_title\": \"string | null\",\n",
    "    \"metadata\": {},\n",
    "    \"sub_organizations\": [],\n",
    "    \"positions\": [\n",
    "      {\n",
    "        \"title_english\": \"string\",\n",
    "        \"title_chinese\": \"string\",\n",
    "        \"metadata\": { \"count\": \"integer | null\", \"list_order_note\": \"string | null\" },\n",
    "        \"personnel\": [\n",
    "          {\n",
    "            \"name_pinyin\": \"string\",\n",
    "            \"name_chinese\": \"string\",\n",
    "            \"dob_year\": \"integer | null\",\n",
    "            \"dob_month\": \"integer | null\",\n",
    "            \"assumed_office_date\": \"YYYY-MM-DD\" | \"YYYY-MM\" | \"YYYY\" | null,\n",
    "            \"cross_reference_symbols\": [\"string\"],\n",
    "            \"gender\": \"male\" | \"female\",\n",
    "            \"ethnicity\": \"string\",\n",
    "            \"rank\": \"string | null\",\n",
    "            \"rank_chinese\": \"string | null\",\n",
    "            \"other_notes\": [\"string\"]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "**Detailed Parsing Rules:**\n",
    "- **Hierarchy:** Capture the nested structure of organizations, sub-organizations, and positions.\n",
    "- **`dob_year` / `dob_month`**: Parse from `(YY.MM)`. A `YY` < 30 is 20xx; otherwise, it's 19xx.\n",
    "- **`assumed_office_date`**: Parse from the `YY.MM.DD` format into ISO 8601 `YYYY-MM-DD`.\n",
    "- **`cross_reference_symbols`**: Collect any leading `☆`, `※`, `◎`, `○` symbols.\n",
    "- **`gender`**: If `(f)` or `(女)` is present, set to \"female\". **Default is \"male\".**\n",
    "- **`ethnicity`**: If an ethnicity like `(Mongolian)` or `(蒙古族)` is present, record it. **Default is \"Han\".**\n",
    "- **`rank` & `rank_chinese`**: Map abbreviations like `(Gen)` to `rank` and the Chinese `(上将)` to `rank_chinese`. **Default is `null` if no rank is specified.**\n",
    "- **`other_notes`**: Place any other parenthetical notes like `(executive)` or `(SPC)` here.\n",
    "- **Continuations:** If the page seems to continue a list from a previous page (e.g., starts with a list of names without a new header), structure the JSON as if it belongs to the last-mentioned organization/position. The top-level object in your response should reflect this context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9ef238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 3\n",
    "def preprocess_pdf_page(page):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF page and reconstructs the two-column layout\n",
    "    into a structured markdown-like format that is easier for the LLM to parse.\n",
    "    \"\"\"\n",
    "    # Get words with coordinates\n",
    "    words = page.get_text(\"words\")\n",
    "    if not words:\n",
    "        return \"\"\n",
    "\n",
    "    # A simple heuristic for the center column split\n",
    "    page_center = page.rect.width / 2\n",
    "\n",
    "    # Group words into lines based on vertical position (y0)\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        x0, y0, x1, y1, text = w[:5]\n",
    "        # Use the integer part of y0 as the key to group words on the same line\n",
    "        y_key = int(y0)\n",
    "        if y_key not in lines:\n",
    "            lines[y_key] = []\n",
    "        lines[y_key].append(w)\n",
    "\n",
    "    # Sort words within each line by their horizontal position (x0)\n",
    "    for y_key in lines:\n",
    "        lines[y_key].sort(key=lambda w: w[0])\n",
    "\n",
    "    # Reconstruct the page text, aligning left and right columns\n",
    "    processed_text = []\n",
    "    sorted_y_keys = sorted(lines.keys())\n",
    "\n",
    "    for y_key in sorted_y_keys:\n",
    "        line_words = lines[y_key]\n",
    "        left_col_text = \" \".join([w[4] for w in line_words if w[0] < page_center])\n",
    "        right_col_text = \" \".join([w[4] for w in line_words if w[2] > page_center])\n",
    "\n",
    "        # Heuristic for detecting headers (usually centered or only in one column)\n",
    "        is_header = (not left_col_text or not right_col_text) and (len(line_words) < 5)\n",
    "\n",
    "        if is_header:\n",
    "            # For headers, just use the full line text\n",
    "            full_line_text = \" \".join([w[4] for w in line_words])\n",
    "            processed_text.append(f\"\\n# {full_line_text}\\n\") # Use markdown header for emphasis\n",
    "        else:\n",
    "            # For data rows, use a clear separator\n",
    "            processed_text.append(f\"{left_col_text.strip()} || {right_col_text.strip()}\")\n",
    "\n",
    "    return \"\\n\".join(processed_text)\n",
    "\n",
    "# In Cell 3\n",
    "def get_json_from_llm(page_text, model_name, host):\n",
    "    \"\"\"\n",
    "    Sends the pre-processed page text to a specific Ollama host and gets a JSON response.\n",
    "    Includes detailed logging and better validation.\n",
    "    \"\"\"\n",
    "    page_text_size = len(page_text)\n",
    "    print(f\"    -> Sending {page_text_size} characters to model '{model_name}'...\")\n",
    "    content = \"\" # Initialize content to avoid reference errors\n",
    "\n",
    "    try:\n",
    "        client = ollama.Client(host=host, timeout=OLLAMA_CLIENT_TIMEOUT)\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': page_text}\n",
    "            ],\n",
    "            options={'temperature': 0.0},\n",
    "            format='json'\n",
    "        )\n",
    "        print(\"    -> Received response from model.\")\n",
    "        content = response['message']['content']\n",
    "        \n",
    "        parsed_json = json.loads(content)\n",
    "\n",
    "        # --- NEW VALIDATION STEP ---\n",
    "        if not isinstance(parsed_json, list):\n",
    "            print(f\"   - ERROR: LLM returned valid JSON, but it's not a LIST as required by the schema. Type was {type(parsed_json)}.\")\n",
    "            print(f\"   - Raw response content was:\\n---\\n{content}\\n---\")\n",
    "            return None # Reject the malformed data\n",
    "        # --- END VALIDATION STEP ---\n",
    "\n",
    "        print(\"    -> Successfully parsed JSON list from response.\")\n",
    "        return parsed_json\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"   - ERROR: Failed to parse JSON from LLM response. Error: {e}\")\n",
    "        print(f\"   - Raw response content was:\\n---\\n{content}\\n---\")\n",
    "        return None\n",
    "    except ollama.ResponseError as e:\n",
    "        print(f\"   - ERROR: An error occurred with the Ollama API: {e.error}\")\n",
    "        print(f\"   - Status code: {e.status_code}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   - ERROR: An unexpected error occurred (likely a timeout or connection issue): {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# In Cell 3\n",
    "def stitch_json_results(all_pages_data):\n",
    "    \"\"\"\n",
    "    Merges the list of JSON objects from each page into a single,\n",
    "    hierarchically correct JSON structure. Now with improved error handling.\n",
    "    \"\"\"\n",
    "    if not all_pages_data: return []\n",
    "    final_data = []\n",
    "    \n",
    "    for i, page_data in enumerate(all_pages_data):\n",
    "        if not page_data: continue\n",
    "\n",
    "        for org_data in page_data:\n",
    "            # --- NEW ROBUSTNESS CHECK ---\n",
    "            # Ensure org_data is a dictionary with the required key\n",
    "            if not isinstance(org_data, dict) or 'organization_name_english' not in org_data:\n",
    "                print(f\"  - WARNING: Skipping malformed organization data on page {i+1}. Data was: {org_data}\")\n",
    "                continue\n",
    "            # --- END ROBUSTNESS CHECK ---\n",
    "\n",
    "            # Check if this organization is a continuation of the previous one\n",
    "            if (final_data and final_data[-1].get('organization_name_english') == org_data.get('organization_name_english')):\n",
    "                last_org = final_data[-1]\n",
    "                if org_data.get('positions'):\n",
    "                    if (last_org.get('positions') and org_data.get('positions') and last_org['positions'][-1].get('title_english') == org_data['positions'][0].get('title_english')):\n",
    "                        last_org['positions'][-1]['personnel'].extend(org_data['positions'][0].get('personnel', []))\n",
    "                        last_org['positions'].extend(org_data['positions'][1:])\n",
    "                    else:\n",
    "                        last_org['positions'].extend(org_data['positions'])\n",
    "                if org_data.get('sub_organizations'):\n",
    "                    last_org['sub_organizations'].extend(org_data.get('sub_organizations', []))\n",
    "            else:\n",
    "                final_data.append(org_data)\n",
    "                \n",
    "    return final_data\n",
    "\n",
    "# In Cell 3, after the stitch_json_results function\n",
    "def save_data_to_file(data, filepath):\n",
    "    \"\"\"Saves the provided data structure to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"  - CRITICAL WARNING: Failed to save data to {filepath}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25fee2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing for '/Users/lukasfiller/dev/china_directory/China Directory 2024-2p5-50.pdf'...\n",
      "Using model: 'llama3.3:70b-instruct-q4_K_M' at host 'http://127.0.0.1:11434'\n",
      "--------------------------------------------------\n",
      "Attempting to connect to Ollama server at http://127.0.0.1:11434...\n",
      "  -> Successfully connected to Ollama server.\n",
      "================================================== \n",
      "!!! SAMPLE MODE: Only processing up to page 5. !!!\n",
      " ==================================================\n",
      "\n",
      "--- Processing Page 1 of 5 ---\n",
      "  - Result: Page 1 already processed. Loading from cache.\n",
      "  - Cached file indicates a previous failure. Re-processing.\n",
      "  - Step 1: Pre-processing page text...\n",
      "  - Step 2: Calling LLM for JSON extraction...\n",
      "    -> Sending 1831 characters to model 'llama3.3:70b-instruct-q4_K_M'...\n",
      "   - ERROR: An unexpected error occurred (likely a timeout or connection issue): timed out\n",
      "  - Step 3: Failed to extract valid JSON from this page. Skipping.\n",
      "--- Page 1 finished in 300.01 seconds ---\n",
      "\n",
      "--- Processing Page 2 of 5 ---\n",
      "  - Result: Page 2 already processed. Loading from cache.\n",
      "  - Cached file indicates a previous failure. Re-processing.\n",
      "  - Step 1: Pre-processing page text...\n",
      "  - Step 2: Calling LLM for JSON extraction...\n",
      "    -> Sending 2056 characters to model 'llama3.3:70b-instruct-q4_K_M'...\n",
      "   - ERROR: An unexpected error occurred (likely a timeout or connection issue): timed out\n",
      "  - Step 3: Failed to extract valid JSON from this page. Skipping.\n",
      "--- Page 2 finished in 300.02 seconds ---\n",
      "\n",
      "--- Processing Page 3 of 5 ---\n",
      "  - Result: Page 3 already processed. Loading from cache.\n",
      "  - Cached file indicates a previous failure. Re-processing.\n",
      "  - Step 1: Pre-processing page text...\n",
      "  - Step 2: Calling LLM for JSON extraction...\n",
      "    -> Sending 2074 characters to model 'llama3.3:70b-instruct-q4_K_M'...\n",
      "   - ERROR: An unexpected error occurred (likely a timeout or connection issue): timed out\n",
      "  - Step 3: Failed to extract valid JSON from this page. Skipping.\n",
      "--- Page 3 finished in 300.05 seconds ---\n",
      "\n",
      "--- Processing Page 4 of 5 ---\n",
      "  - Result: Page 4 already processed. Loading from cache.\n",
      "  - Cached file indicates a previous failure. Re-processing.\n",
      "  - Step 1: Pre-processing page text...\n",
      "  - Step 2: Calling LLM for JSON extraction...\n",
      "    -> Sending 2346 characters to model 'llama3.3:70b-instruct-q4_K_M'...\n",
      "   - ERROR: An unexpected error occurred (likely a timeout or connection issue): timed out\n",
      "  - Step 3: Failed to extract valid JSON from this page. Skipping.\n",
      "--- Page 4 finished in 300.03 seconds ---\n",
      "\n",
      "--- Processing Page 5 of 5 ---\n",
      "  - Result: Page 5 already processed. Loading from cache.\n",
      "  - Cached file indicates a previous failure. Re-processing.\n",
      "  - Step 1: Pre-processing page text...\n",
      "  - Step 2: Calling LLM for JSON extraction...\n",
      "    -> Sending 2628 characters to model 'llama3.3:70b-instruct-q4_K_M'...\n",
      "   - ERROR: An unexpected error occurred (likely a timeout or connection issue): timed out\n",
      "  - Step 3: Failed to extract valid JSON from this page. Skipping.\n",
      "--- Page 5 finished in 300.02 seconds ---\n",
      "\n",
      "==================================================\n",
      "All pages processed or loaded from cache. Now stitching final results...\n",
      "Stitching complete.\n",
      "Writing final structured data to 'output.json'...\n",
      "\n",
      "==================================================\n",
      "✅ PROCESSING COMPLETE ✅\n",
      "Attempted to process 5 page(s).\n",
      "Successfully processed and extracted data from 0 page(s).\n",
      "Total execution time: 1500.15 seconds.\n",
      "Final output saved to: output.json\n",
      "WARNING: No data was successfully extracted. output.json will contain an empty list.\n"
     ]
    }
   ],
   "source": [
    "# In Cell 4 (Execution)\n",
    "import time\n",
    "\n",
    "print(f\"Starting processing for '{PDF_PATH}'...\")\n",
    "print(f\"Using model: '{MODEL_NAME}' at host '{OLLAMA_HOST}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- RESUME LOGIC: Load existing data if it exists ---\n",
    "all_pages_data = []\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    print(f\"Found existing output file at '{OUTPUT_PATH}'. Attempting to load and resume.\")\n",
    "    try:\n",
    "        with open(OUTPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "            # Note: We load the raw page-by-page data, not the stitched version.\n",
    "            # This is a simplification; a more complex resume would track page numbers.\n",
    "            # For this script, we'll re-stitch every time, which is safer.\n",
    "            # We'll re-implement the resume logic based on pages processed.\n",
    "            # For now, let's start fresh but save incrementally.\n",
    "            # A better approach is to check how many pages are done.\n",
    "            # Let's re-architect for a simple incremental save first.\n",
    "            print(\"Starting a fresh run, but will save progress incrementally.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - WARNING: Could not read existing file. Starting fresh. Error: {e}\")\n",
    "# --- Let's refine this resume logic to be more effective ---\n",
    "\n",
    "all_pages_data = [] # This will hold the JSON from each page\n",
    "start_page = 0      # Default to starting from the beginning\n",
    "\n",
    "# A better resume strategy:\n",
    "# We will create a temporary cache directory to store the result of each page.\n",
    "CACHE_DIR = \"page_cache\"\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "# --- NEW: OLLAMA SERVER PRE-CHECK ---\n",
    "print(f\"Attempting to connect to Ollama server at {OLLAMA_HOST}...\")\n",
    "try:\n",
    "    client = ollama.Client(host=OLLAMA_HOST, timeout=10) # Short timeout for pre-check\n",
    "    client.list() # A lightweight command to check connectivity\n",
    "    print(\"  -> Successfully connected to Ollama server.\")\n",
    "except Exception as e:\n",
    "    print(f\"  - FATAL ERROR: Could not connect to Ollama server at {OLLAMA_HOST}.\")\n",
    "    print(f\"  - Error details: {e}\")\n",
    "    print(\"  - Please ensure Ollama is running and accessible.\")\n",
    "    # Exit the script if Ollama is not available\n",
    "    # In a notebook environment, this might not stop execution of subsequent cells directly,\n",
    "    # so we'll rely on a flag or simply not proceed with PDF processing.\n",
    "    ollama_available = False\n",
    "else:\n",
    "    ollama_available = True\n",
    "\n",
    "if ollama_available:\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"FATAL ERROR: PDF file not found at '{PDF_PATH}'\")\n",
    "    else:\n",
    "        doc = fitz.open(PDF_PATH)\n",
    "        num_pages = len(doc)\n",
    "        \n",
    "        pages_to_run = num_pages\n",
    "        if MAX_PAGES_TO_PROCESS is not None and MAX_PAGES_TO_PROCESS < num_pages:\n",
    "            print(\"=\" * 50, f\"\\n!!! SAMPLE MODE: Only processing up to page {MAX_PAGES_TO_PROCESS}. !!!\\n\", \"=\" * 50)\n",
    "            pages_to_run = MAX_PAGES_TO_PROCESS\n",
    "\n",
    "        total_start_time = time.time()\n",
    "        successful_pages_count = 0\n",
    "        attempted_pages_count = 0\n",
    "        \n",
    "        # --- MAIN LOOP WITH INCREMENTAL SAVING ---\n",
    "        for i in range(pages_to_run):\n",
    "            page_num = i + 1\n",
    "            page_cache_path = os.path.join(CACHE_DIR, f\"page_{page_num}.json\")\n",
    "            page_successfully_processed_or_cached = False # Flag for this page\n",
    "            \n",
    "            print(f\"\\n--- Processing Page {page_num} of {pages_to_run} ---\")\n",
    "\n",
    "            # --- RESUME LOGIC: Check if page is already cached ---\n",
    "            if os.path.exists(page_cache_path):\n",
    "                print(f\"  - Result: Page {page_num} already processed. Loading from cache.\")\n",
    "                try:\n",
    "                    with open(page_cache_path, 'r', encoding='utf-8') as f:\n",
    "                        cached_data = json.load(f)\n",
    "                    if cached_data is None: # Indicates a previous failure\n",
    "                        print(\"  - Cached file indicates a previous failure. Re-processing.\")\n",
    "                        # Do not append to all_pages_data yet, proceed to reprocessing\n",
    "                    else:\n",
    "                        print(\"  - Successfully loaded valid data from cache.\")\n",
    "                        all_pages_data.append(cached_data) # Add valid cached data\n",
    "                        successful_pages_count += 1\n",
    "                        page_successfully_processed_or_cached = True\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"  - WARNING: Cache file is corrupted. Re-processing.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  - WARNING: Could not load from cache ({e}). Re-processing.\")\n",
    "\n",
    "            if page_successfully_processed_or_cached:\n",
    "                continue # Skip to the next page\n",
    "            \n",
    "            # --- If not cached, or cache indicated failure, process the page normally ---\n",
    "            page_start_time = time.time()\n",
    "            page = doc[i]\n",
    "            \n",
    "            print(\"  - Step 1: Pre-processing page text...\")\n",
    "            page_text = preprocess_pdf_page(page)\n",
    "            \n",
    "            if not page_text.strip():\n",
    "                print(\"  - Result: Page is empty, skipping.\")\n",
    "                # Cache the empty result so we don't re-process it\n",
    "                with open(page_cache_path, 'w', encoding='utf-8') as f: json.dump(None, f)\n",
    "                continue\n",
    "            \n",
    "            print(\"  - Step 2: Calling LLM for JSON extraction...\")\n",
    "            page_json = get_json_from_llm(page_text, MODEL_NAME, OLLAMA_HOST)\n",
    "            \n",
    "            # Cache the result, whether it's successful (JSON) or a failure (None)\n",
    "            with open(page_cache_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(page_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            if page_json:\n",
    "                all_pages_data.append(page_json)\n",
    "                successful_pages_count += 1\n",
    "                num_orgs = len(page_json)\n",
    "                print(f\"  - Step 3: Success! Extracted {num_orgs} top-level organization(s) from page.\")\n",
    "                if DEBUG_PRINT_JSON:\n",
    "                    print(\"    -- Debug: JSON output for this page --\"); print(json.dumps(page_json, ensure_ascii=False, indent=2)); print(\"    -- End Debug --\")\n",
    "            else:\n",
    "                print(\"  - Step 3: Failed to extract valid JSON from this page. Skipping.\")\n",
    "            \n",
    "            page_end_time = time.time()\n",
    "            page_duration = page_end_time - page_start_time\n",
    "            print(f\"--- Page {page_num} finished in {page_duration:.2f} seconds ---\")\n",
    "\n",
    "        # --- FINAL STITCHING AND SAVING ---\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"All pages processed or loaded from cache. Now stitching final results...\")\n",
    "    \n",
    "    final_stitched_data = stitch_json_results(all_pages_data)\n",
    "    print(\"Stitching complete.\")\n",
    "\n",
    "    print(f\"Writing final structured data to '{OUTPUT_PATH}'...\")\n",
    "    save_data_to_file(final_stitched_data, OUTPUT_PATH)\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"✅ PROCESSING COMPLETE ✅\")\n",
    "    print(f\"Attempted to process {pages_to_run} page(s).\")\n",
    "    print(f\"Successfully processed and extracted data from {successful_pages_count} page(s).\")\n",
    "    print(f\"Total execution time: {total_duration:.2f} seconds.\")\n",
    "    print(f\"Final output saved to: {OUTPUT_PATH}\")\n",
    "    if successful_pages_count == 0 and pages_to_run > 0:\n",
    "        print(f\"WARNING: No data was successfully extracted. {OUTPUT_PATH} will contain an empty list.\")\n",
    "elif not ollama_available:\n",
    "    print(\"Skipped PDF processing as Ollama server was not available.\")\n",
    "else: # Ollama available, but PDF might have been missing or other setup issue\n",
    "    print(\"Skipped PDF processing due to missing PDF file or other setup issue prior to page loop.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "china_dir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
