{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f2cbf1",
   "metadata": {},
   "source": [
    "in terminal:\n",
    "conda init\n",
    "conda activate china_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U PyMuPDF\n",
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b891c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d267b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lukasfiller/dev/china_directory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- START OF CONFIGURATION ---\n",
    "# Change these values for your specific run\n",
    "PDF_PATH = \"/Users/lukasfiller/dev/china_directory/China Directory 2024-2p5-50.pdf\"\n",
    "OUTPUT_PATH = \"output.json\"\n",
    "MODEL_NAME = \"llama3.1:8b-instruct-fp16\"\n",
    "OLLAMA_HOST = \"http://127.0.0.1:11434\"\n",
    "OLLAMA_CLIENT_TIMEOUT = 600  # Seconds\n",
    "DEBUG_PRINT_JSON = True\n",
    "\n",
    "# --- NEW: Set a limit for testing ---\n",
    "# Set to a number (e.g., 5) for a sample run, or None to process the whole document.\n",
    "MAX_PAGES_TO_PROCESS = 2 \n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# This is the instruction set we developed, telling the LLM its role and rules.\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Extract information from Chinese government directory text and return as JSON array.\n",
    "\n",
    "INPUT FORMAT: Text with \"||\" separating left/right columns, names with birth dates like (YY.MM)\n",
    "\n",
    "OUTPUT: JSON array with this structure:\n",
    "[\n",
    "  {\n",
    "    \"organization_name_english\": \"State Council\",\n",
    "    \"organization_name_chinese\": \"国务院\", \n",
    "    \"document_section_title\": null,\n",
    "    \"metadata\": {},\n",
    "    \"sub_organizations\": [],\n",
    "    \"positions\": [\n",
    "      {\n",
    "        \"title_english\": \"Premier\",\n",
    "        \"title_chinese\": \"总理\",\n",
    "        \"metadata\": {\"count\": null, \"list_order_note\": null},\n",
    "        \"personnel\": [\n",
    "          {\n",
    "            \"name_pinyin\": \"Li Keqiang\",\n",
    "            \"name_chinese\": \"李克强\",\n",
    "            \"dob_year\": 1955,\n",
    "            \"dob_month\": 3,\n",
    "            \"assumed_office_date\": null,\n",
    "            \"cross_reference_symbols\": [],\n",
    "            \"gender\": \"male\",\n",
    "            \"ethnicity\": \"Han\",\n",
    "            \"rank\": null,\n",
    "            \"rank_chinese\": null,\n",
    "            \"other_notes\": []\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "RULES:\n",
    "- Extract person names and birth dates from (YY.MM) format\n",
    "- YY < 30 = 20XX, YY >= 30 = 19XX\n",
    "- Default gender: \"male\", ethnicity: \"Han\"\n",
    "- Return ONLY valid JSON array, no explanations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9ef238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 3\n",
    "def preprocess_pdf_page(page):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF page and reconstructs the two-column layout\n",
    "    into a structured markdown-like format that is easier for the LLM to parse.\n",
    "    \"\"\"\n",
    "    # Get words with coordinates\n",
    "    words = page.get_text(\"words\")\n",
    "    if not words:\n",
    "        return \"\"\n",
    "\n",
    "    # A simple heuristic for the center column split\n",
    "    page_center = page.rect.width / 2\n",
    "\n",
    "    # Group words into lines based on vertical position (y0)\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        x0, y0, x1, y1, text = w[:5]\n",
    "        # Use the integer part of y0 as the key to group words on the same line\n",
    "        y_key = int(y0)\n",
    "        if y_key not in lines:\n",
    "            lines[y_key] = []\n",
    "        lines[y_key].append(w)\n",
    "\n",
    "    # Sort words within each line by their horizontal position (x0)\n",
    "    for y_key in lines:\n",
    "        lines[y_key].sort(key=lambda w: w[0])\n",
    "\n",
    "    # Reconstruct the page text, aligning left and right columns\n",
    "    processed_text = []\n",
    "    sorted_y_keys = sorted(lines.keys())\n",
    "\n",
    "    for y_key in sorted_y_keys:\n",
    "        line_words = lines[y_key]\n",
    "        left_col_text = \" \".join([w[4] for w in line_words if w[0] < page_center])\n",
    "        right_col_text = \" \".join([w[4] for w in line_words if w[2] > page_center])\n",
    "\n",
    "        # Heuristic for detecting headers (usually centered or only in one column)\n",
    "        is_header = (not left_col_text or not right_col_text) and (len(line_words) < 5)\n",
    "\n",
    "        if is_header:\n",
    "            # For headers, just use the full line text\n",
    "            full_line_text = \" \".join([w[4] for w in line_words])\n",
    "            processed_text.append(f\"\\n# {full_line_text}\\n\") # Use markdown header for emphasis\n",
    "        else:\n",
    "            # For data rows, use a clear separator\n",
    "            processed_text.append(f\"{left_col_text.strip()} || {right_col_text.strip()}\")\n",
    "\n",
    "    return \"\\n\".join(processed_text)\n",
    "\n",
    "# In Cell 3\n",
    "def get_json_from_llm(page_text, model_name, host):\n",
    "    \"\"\"\n",
    "    Sends the pre-processed page text to a specific Ollama host and gets a JSON response.\n",
    "    \"\"\"\n",
    "    page_text_size = len(page_text)\n",
    "    print(f\"    -> Sending {page_text_size} characters to model '{model_name}'...\")\n",
    "    content = \"\"\n",
    "\n",
    "    try:\n",
    "        client = ollama.Client(host=host, timeout=OLLAMA_CLIENT_TIMEOUT)\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': page_text}\n",
    "            ],\n",
    "            options={'temperature': 0.0}\n",
    "            # REMOVED: format='json' - this was causing issues\n",
    "        )\n",
    "        print(\"    -> Received response from model.\")\n",
    "        content = response['message']['content']\n",
    "        \n",
    "        print(f\"    -> Raw response (first 200 chars): {content[:200]}\")\n",
    "        \n",
    "        # Try to extract JSON from the response\n",
    "        content = content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "        content = content.strip()\n",
    "        \n",
    "        # Try to find JSON array in the response\n",
    "        import re\n",
    "        json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "        if json_match:\n",
    "            content = json_match.group(0)\n",
    "        \n",
    "        parsed_json = json.loads(content)\n",
    "\n",
    "        if isinstance(parsed_json, dict):\n",
    "            print(\"    -> LLM returned a dictionary. Wrapping it in a list.\")\n",
    "            return [parsed_json]\n",
    "        elif isinstance(parsed_json, list):\n",
    "            print(\"    -> Successfully parsed JSON list from response.\")\n",
    "            return parsed_json\n",
    "        else:\n",
    "            print(f\"   - ERROR: Unexpected JSON type: {type(parsed_json)}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"   - ERROR: Failed to parse JSON. Error: {e}\")\n",
    "        print(f\"   - Content: {content}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   - ERROR: {e}\")\n",
    "        return None\n",
    "        \n",
    "# In Cell 3\n",
    "def stitch_json_results(all_pages_data):\n",
    "    \"\"\"\n",
    "    Merges the list of JSON objects from each page into a single,\n",
    "    hierarchically correct JSON structure. Now with improved error handling.\n",
    "    \"\"\"\n",
    "    if not all_pages_data: return []\n",
    "    final_data = []\n",
    "    \n",
    "    for i, page_data in enumerate(all_pages_data):\n",
    "        if not page_data: continue\n",
    "\n",
    "        for org_data in page_data:\n",
    "            # --- NEW ROBUSTNESS CHECK ---\n",
    "            # Ensure org_data is a dictionary with the required key\n",
    "            if not isinstance(org_data, dict) or 'organization_name_english' not in org_data:\n",
    "                print(f\"  - WARNING: Skipping malformed organization data on page {i+1}. Data was: {org_data}\")\n",
    "                continue\n",
    "            # --- END ROBUSTNESS CHECK ---\n",
    "\n",
    "            # Check if this organization is a continuation of the previous one\n",
    "            if (final_data and final_data[-1].get('organization_name_english') == org_data.get('organization_name_english')):\n",
    "                last_org = final_data[-1]\n",
    "                if org_data.get('positions'):\n",
    "                    if (last_org.get('positions') and org_data.get('positions') and last_org['positions'][-1].get('title_english') == org_data['positions'][0].get('title_english')):\n",
    "                        last_org['positions'][-1]['personnel'].extend(org_data['positions'][0].get('personnel', []))\n",
    "                        last_org['positions'].extend(org_data['positions'][1:])\n",
    "                    else:\n",
    "                        last_org['positions'].extend(org_data['positions'])\n",
    "                if org_data.get('sub_organizations'):\n",
    "                    last_org['sub_organizations'].extend(org_data.get('sub_organizations', []))\n",
    "            else:\n",
    "                final_data.append(org_data)\n",
    "                \n",
    "    return final_data\n",
    "\n",
    "# In Cell 3, after the stitch_json_results function\n",
    "def save_data_to_file(data, filepath):\n",
    "    \"\"\"Saves the provided data structure to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"  - CRITICAL WARNING: Failed to save data to {filepath}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caa68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 4 (Execution)\n",
    "import time\n",
    "\n",
    "print(f\"Starting processing for '{PDF_PATH}'...\")\n",
    "print(f\"Using model: '{MODEL_NAME}' at host '{OLLAMA_HOST}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- RESUME LOGIC: Load existing data if it exists ---\n",
    "all_pages_data = []\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    print(f\"Found existing output file at '{OUTPUT_PATH}'. Attempting to load and resume.\")\n",
    "    try:\n",
    "        with open(OUTPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "            print(\"Starting a fresh run, but will save progress incrementally.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - WARNING: Could not read existing file. Starting fresh. Error: {e}\")\n",
    "\n",
    "all_pages_data = [] # This will hold the JSON from each page\n",
    "start_page = 0      # Default to starting from the beginning\n",
    "\n",
    "# A better resume strategy:\n",
    "# We will create a temporary cache directory to store the result of each page.\n",
    "CACHE_DIR = \"page_cache\"\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "# --- NEW: OLLAMA SERVER PRE-CHECK ---\n",
    "print(f\"Attempting to connect to Ollama server at {OLLAMA_HOST}...\")\n",
    "try:\n",
    "    client = ollama.Client(host=OLLAMA_HOST, timeout=10) # Short timeout for pre-check\n",
    "    client.list() # A lightweight command to check connectivity\n",
    "    print(\"  -> Successfully connected to Ollama server.\")\n",
    "except Exception as e:\n",
    "    print(f\"  - FATAL ERROR: Could not connect to Ollama server at {OLLAMA_HOST}.\")\n",
    "    print(f\"  - Error details: {e}\")\n",
    "    print(\"  - Please ensure Ollama is running and accessible.\")\n",
    "    ollama_available = False\n",
    "else:\n",
    "    ollama_available = True\n",
    "\n",
    "if ollama_available:\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"FATAL ERROR: PDF file not found at '{PDF_PATH}'\")\n",
    "    else:\n",
    "        doc = fitz.open(PDF_PATH)\n",
    "        num_pages = len(doc)\n",
    "        \n",
    "        pages_to_run = num_pages\n",
    "        if MAX_PAGES_TO_PROCESS is not None and MAX_PAGES_TO_PROCESS < num_pages:\n",
    "            print(\"=\" * 50, f\"\\n!!! SAMPLE MODE: Only processing up to page {MAX_PAGES_TO_PROCESS}. !!!\\n\", \"=\" * 50)\n",
    "            pages_to_run = MAX_PAGES_TO_PROCESS\n",
    "\n",
    "        total_start_time = time.time()\n",
    "        successful_pages_count = 0\n",
    "        attempted_pages_count = 0\n",
    "        \n",
    "        # --- MAIN LOOP WITH INCREMENTAL SAVING ---\n",
    "        for i in range(pages_to_run):\n",
    "            page_num = i + 1\n",
    "            page_cache_path = os.path.join(CACHE_DIR, f\"page_{page_num}.json\")\n",
    "            page_successfully_processed_or_cached = False # Flag for this page\n",
    "            \n",
    "            print(f\"\\n--- Processing Page {page_num} of {pages_to_run} ---\")\n",
    "\n",
    "            # --- RESUME LOGIC: Check if page is already cached ---\n",
    "            if os.path.exists(page_cache_path):\n",
    "                print(f\"  - Result: Page {page_num} already processed. Loading from cache.\")\n",
    "                try:\n",
    "                    with open(page_cache_path, 'r', encoding='utf-8') as f:\n",
    "                        cached_data = json.load(f)\n",
    "                    if cached_data is None: # Indicates a previous failure\n",
    "                        print(\"  - Cached file indicates a previous failure. Re-processing.\")\n",
    "                        # Do not append to all_pages_data yet, proceed to reprocessing\n",
    "                    else:\n",
    "                        print(\"  - Successfully loaded valid data from cache.\")\n",
    "                        all_pages_data.append(cached_data) # Add valid cached data\n",
    "                        successful_pages_count += 1\n",
    "                        page_successfully_processed_or_cached = True\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"  - WARNING: Cache file is corrupted. Re-processing.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  - WARNING: Could not load from cache ({e}). Re-processing.\")\n",
    "\n",
    "            if page_successfully_processed_or_cached:\n",
    "                continue # Skip to the next page\n",
    "            \n",
    "            # --- If not cached, or cache indicated failure, process the page normally ---\n",
    "            page_start_time = time.time()\n",
    "            page = doc[i]\n",
    "            \n",
    "            print(\"  - Step 1: Pre-processing page text...\")\n",
    "            page_text = preprocess_pdf_page(page)\n",
    "            \n",
    "            if not page_text.strip():\n",
    "                print(\"  - Result: Page is empty, skipping.\")\n",
    "                # Cache the empty result so we don't re-process it\n",
    "                with open(page_cache_path, 'w', encoding='utf-8') as f: json.dump(None, f)\n",
    "                continue\n",
    "            \n",
    "            print(\"  - Step 2: Calling LLM for JSON extraction...\")\n",
    "            print(\"--- DEBUG: Preprocessed Page Text START ---\")\n",
    "            print(page_text)\n",
    "            print(\"--- DEBUG: Preprocessed Page Text END ---\")\n",
    "\n",
    "            page_json = get_json_from_llm(page_text, MODEL_NAME, OLLAMA_HOST)\n",
    "            \n",
    "            # Cache the result, whether it's successful (JSON) or a failure (None)\n",
    "            with open(page_cache_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(page_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            if page_json:\n",
    "                all_pages_data.append(page_json)\n",
    "                successful_pages_count += 1\n",
    "                num_orgs = len(page_json)\n",
    "                print(f\"  - Step 3: Success! Extracted {num_orgs} top-level organization(s) from page.\")\n",
    "                if DEBUG_PRINT_JSON:\n",
    "                    print(\"    -- Debug: JSON output for this page --\"); print(json.dumps(page_json, ensure_ascii=False, indent=2)); print(\"    -- End Debug --\")\n",
    "            else:\n",
    "                print(\"  - Step 3: Failed to extract valid JSON from this page. Skipping.\")\n",
    "            \n",
    "            page_end_time = time.time()\n",
    "            page_duration = page_end_time - page_start_time\n",
    "            print(f\"--- Page {page_num} finished in {page_duration:.2f} seconds ---\")\n",
    "\n",
    "        # --- FINAL STITCHING AND SAVING --- (FIX: Moved this inside the else block)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"All pages processed or loaded from cache. Now stitching final results...\")\n",
    "        \n",
    "        final_stitched_data = stitch_json_results(all_pages_data)\n",
    "        print(\"Stitching complete.\")\n",
    "\n",
    "        print(f\"Writing final structured data to '{OUTPUT_PATH}'...\")\n",
    "        save_data_to_file(final_stitched_data, OUTPUT_PATH)\n",
    "\n",
    "        total_end_time = time.time()\n",
    "        total_duration = total_end_time - total_start_time\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"✅ PROCESSING COMPLETE ✅\")\n",
    "        print(f\"Attempted to process {pages_to_run} page(s).\")\n",
    "        print(f\"Successfully processed and extracted data from {successful_pages_count} page(s).\")\n",
    "        print(f\"Total execution time: {total_duration:.2f} seconds.\")\n",
    "        print(f\"Final output saved to: {OUTPUT_PATH}\")\n",
    "        if successful_pages_count == 0 and pages_to_run > 0:\n",
    "            print(f\"WARNING: No data was successfully extracted. {OUTPUT_PATH} will contain an empty list.\")\n",
    "elif not ollama_available:\n",
    "    print(\"Skipped PDF processing as Ollama server was not available.\")\n",
    "else: # Ollama available, but PDF might have been missing or other setup issue\n",
    "    print(\"Skipped PDF processing due to missing PDF file or other setup issue prior to page loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debugging cell to check the cache contents\n",
    "import json\n",
    "import os\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "print(\"=== CACHE DIRECTORY CONTENTS ===\")\n",
    "\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    cache_files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.json')]\n",
    "    print(f\"Found {len(cache_files)} cache files: {cache_files}\")\n",
    "    \n",
    "    for cache_file in sorted(cache_files):\n",
    "        cache_path = os.path.join(CACHE_DIR, cache_file)\n",
    "        print(f\"\\n--- {cache_file} ---\")\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if data is None:\n",
    "                print(\"  Content: None (indicates processing failure)\")\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"  Content: List with {len(data)} items\")\n",
    "                if data:\n",
    "                    print(f\"  First item keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not a dict'}\")\n",
    "            else:\n",
    "                print(f\"  Content: {type(data)} - {str(data)[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading file: {e}\")\n",
    "else:\n",
    "    print(\"Cache directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a183ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this enhanced debugging cell to see the actual content\n",
    "import json\n",
    "import os\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "print(\"=== DETAILED CACHE ANALYSIS ===\")\n",
    "\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    cache_files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.json')]\n",
    "    print(f\"Found {len(cache_files)} cache files: {cache_files}\")\n",
    "    \n",
    "    for cache_file in sorted(cache_files):\n",
    "        cache_path = os.path.join(CACHE_DIR, cache_file)\n",
    "        print(f\"\\n--- {cache_file} ---\")\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if data is None:\n",
    "                print(\"  Content: None (indicates processing failure)\")\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"  Content: List with {len(data)} items\")\n",
    "                for i, item in enumerate(data):\n",
    "                    print(f\"  Item {i+1}: {type(item)}\")\n",
    "                    if isinstance(item, dict):\n",
    "                        print(f\"    Keys: {list(item.keys())}\")\n",
    "                        print(f\"    Full content: {json.dumps(item, ensure_ascii=False, indent=4)}\")\n",
    "                    else:\n",
    "                        print(f\"    Content: {item}\")\n",
    "            else:\n",
    "                print(f\"  Content: {type(data)} - {str(data)[:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading file: {e}\")\n",
    "else:\n",
    "    print(\"Cache directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd943e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM response with a simple example\n",
    "import ollama\n",
    "\n",
    "test_text = \"\"\"\n",
    "# State Council\n",
    "Premier Li Keqiang (65.03) || Deputy Premier Han Zheng (68.04)\n",
    "State Councilor Wei Fenghe (67.02) || State Councilor Wang Yi (69.10)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== TESTING LLM RESPONSE ===\")\n",
    "print(f\"Input text:\\n{test_text}\")\n",
    "\n",
    "try:\n",
    "    client = ollama.Client(host=OLLAMA_HOST, timeout=60)\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': test_text}\n",
    "        ],\n",
    "        options={'temperature': 0.0},\n",
    "        format='json'\n",
    "    )\n",
    "    \n",
    "    content = response['message']['content']\n",
    "    print(f\"\\nRaw LLM response:\\n{content}\")\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(content)\n",
    "        print(f\"\\nParsed JSON type: {type(parsed)}\")\n",
    "        print(f\"Parsed JSON content:\\n{json.dumps(parsed, ensure_ascii=False, indent=2)}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\nJSON parsing failed: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce40ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache cell\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(\"Cache cleared\")\n",
    "    \n",
    "# Also clear output file\n",
    "if os.path.exists(\"output.json\"):\n",
    "    os.remove(\"output.json\")\n",
    "    print(\"Output file cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "china_dir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
