{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f2cbf1",
   "metadata": {},
   "source": [
    "in terminal:\n",
    "conda init\n",
    "conda activate china_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U PyMuPDF\n",
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b891c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d267b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lukasfiller/dev/china_directory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your configuration cell, replace the MAX_PAGES_TO_PROCESS section with this:\n",
    "# --- START OF CONFIGURATION ---\n",
    "# Change these values for your specific run\n",
    "PDF_PATH = \"/Users/lukasfiller/dev/china_directory/China Directory 2024-2p5-50.pdf\"\n",
    "OUTPUT_PATH = \"output.json\"\n",
    "MODEL_NAME = \"llama3.1:8b-instruct-fp16\"\n",
    "OLLAMA_HOST = \"http://127.0.0.1:11434\"\n",
    "OLLAMA_CLIENT_TIMEOUT = 600  # Seconds\n",
    "DEBUG_PRINT_JSON = True\n",
    "\n",
    "# --- NEW: Page Range Control ---\n",
    "# Set specific page ranges for batch processing\n",
    "START_PAGE = 1      # First page to process (1-indexed)\n",
    "END_PAGE = 5        # Last page to process (inclusive, 1-indexed)\n",
    "# Set both to None to process entire document\n",
    "# Examples:\n",
    "# START_PAGE = 1, END_PAGE = 10    # Process pages 1-10\n",
    "# START_PAGE = 11, END_PAGE = 20   # Process pages 11-20\n",
    "# START_PAGE = None, END_PAGE = None # Process entire document\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# This is the instruction set we developed, telling the LLM its role and rules.\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert AI data extraction agent. Your sole purpose is to analyze a single page from a specific multi-lingual PDF document about government personnel and convert its contents into a structured JSON array.\n",
    "\n",
    "The source is a formal personnel directory with a three-column layout. The left column contains English titles and Pinyin names. The middle column contains Chinese/Japanese titles and names, often with parenthetical metadata. The right column contains dates.\n",
    "\n",
    "OUTPUT: Single valid JSON array following this schema:\n",
    "[\n",
    "  {\n",
    "    \"source_pdf_page\": integer,\n",
    "    \"organization_name_english\": \"string\",\n",
    "    \"organization_name_chinese\": \"string\", \n",
    "    \"metadata\": {\n",
    "      \"establishment_date\": \"string | null\",\n",
    "      \"list_order_note\": \"string | null\",\n",
    "      \"count\": \"integer | null\"\n",
    "    },\n",
    "    \"positions\": [\n",
    "      {\n",
    "        \"title_english\": \"string\",\n",
    "        \"title_chinese\": \"string\",\n",
    "        \"personnel\": [\n",
    "          {\n",
    "            \"name_pinyin\": \"string\",\n",
    "            \"name_chinese\": \"string\",\n",
    "            \"raw_cn_jp_entry\": \"string\",\n",
    "            \"assumed_office_date\": \"string | null\",\n",
    "            \"birth_year\": \"integer | null\",\n",
    "            \"birth_month\": \"integer | null\", \n",
    "            \"birth_day\": \"integer | null\",\n",
    "            \"cross_reference_symbol\": \"string | null\",\n",
    "            \"gender\": \"string | null\",\n",
    "            \"ethnicity\": \"string | null\",\n",
    "            \"rank_english\": \"string | null\",\n",
    "            \"rank_chinese\": \"string | null\",\n",
    "            \"other_notes_en\": [],\n",
    "            \"other_notes_cn_jp\": []\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"sub_organizations\": []\n",
    "  }\n",
    "]\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Data in parentheses () next to names is EXCLUSIVELY personal metadata (birth date, rank, gender, etc.)\n",
    "2. Date in far right column is EXCLUSIVELY the assumed_office_date\n",
    "3. Extract symbols ☆, ※, ◎, ○ from names to cross_reference_symbol field\n",
    "4. Parse (f) or (女) -> gender: \"female\", default is \"male\"\n",
    "5. Parse (Gen) or (上将) -> rank fields\n",
    "6. Parse (Tujia) or (土家族) -> ethnicity, default is \"Han\"\n",
    "7. Always include raw_cn_jp_entry with complete unmodified Chinese/Japanese string\n",
    "8. Handle one-to-many mappings where one English name maps to multiple Chinese names\n",
    "\n",
    "Return ONLY the JSON array, no explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ef238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 3\n",
    "def preprocess_pdf_page(page):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF page and reconstructs the three-column layout\n",
    "    (English/Pinyin, Chinese/Japanese, Dates) with proper column association.\n",
    "    \"\"\"\n",
    "    words = page.get_text(\"words\")\n",
    "    if not words:\n",
    "        return \"\"\n",
    "\n",
    "    # Sort words by vertical position first, then horizontal\n",
    "    words.sort(key=lambda w: (w[1], w[0]))\n",
    "    \n",
    "    # Determine column boundaries (assuming 3 columns)\n",
    "    page_width = page.rect.width\n",
    "    col1_boundary = page_width * 0.33  # English/Pinyin\n",
    "    col2_boundary = page_width * 0.67  # Chinese/Japanese\n",
    "    # col3 is dates (remaining right side)\n",
    "    \n",
    "    # Group words into lines and columns\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        x0, y0, x1, y1, text = w[:5]\n",
    "        y_key = round(y0 / 5) * 5  # Group nearby lines\n",
    "        \n",
    "        if y_key not in lines:\n",
    "            lines[y_key] = {'col1': [], 'col2': [], 'col3': []}\n",
    "        \n",
    "        if x0 < col1_boundary:\n",
    "            lines[y_key]['col1'].append(text)\n",
    "        elif x0 < col2_boundary:\n",
    "            lines[y_key]['col2'].append(text)\n",
    "        else:\n",
    "            lines[y_key]['col3'].append(text)\n",
    "    \n",
    "    # Reconstruct with proper column indicators\n",
    "    processed_text = []\n",
    "    for y_key in sorted(lines.keys()):\n",
    "        col1_text = \" \".join(lines[y_key]['col1']).strip()\n",
    "        col2_text = \" \".join(lines[y_key]['col2']).strip()\n",
    "        col3_text = \" \".join(lines[y_key]['col3']).strip()\n",
    "        \n",
    "        # Only add non-empty lines\n",
    "        if col1_text or col2_text or col3_text:\n",
    "            line_parts = []\n",
    "            if col1_text: line_parts.append(f\"EN: {col1_text}\")\n",
    "            if col2_text: line_parts.append(f\"CN: {col2_text}\")\n",
    "            if col3_text: line_parts.append(f\"DATE: {col3_text}\")\n",
    "            \n",
    "            if line_parts:\n",
    "                processed_text.append(\" | \".join(line_parts))\n",
    "    \n",
    "    return \"\\n\".join(processed_text)\n",
    "\n",
    "# In Cell 3\n",
    "def get_json_from_llm(page_text, model_name, host, page_num):\n",
    "    \"\"\"\n",
    "    Sends the pre-processed page text to a specific Ollama host and gets a JSON response.\n",
    "    \"\"\"\n",
    "    page_text_size = len(page_text)\n",
    "    print(f\"    -> Sending {page_text_size} characters to model '{model_name}'...\")\n",
    "    \n",
    "    # Add page number context to the prompt\n",
    "    user_prompt = f\"PAGE {page_num}:\\n{page_text}\"\n",
    "    \n",
    "    try:\n",
    "        client = ollama.Client(host=host, timeout=OLLAMA_CLIENT_TIMEOUT)\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ],\n",
    "            options={'temperature': 0.0},\n",
    "            format='json'\n",
    "        )\n",
    "        \n",
    "        content = response['message']['content']\n",
    "        print(f\"    -> Raw response (first 200 chars): {content[:200]}\")\n",
    "        \n",
    "        parsed_json = json.loads(content)\n",
    "\n",
    "        # Ensure page number is set in the response\n",
    "        if isinstance(parsed_json, list):\n",
    "            for org in parsed_json:\n",
    "                if isinstance(org, dict):\n",
    "                    org['source_pdf_page'] = page_num\n",
    "        \n",
    "        if isinstance(parsed_json, dict):\n",
    "            parsed_json['source_pdf_page'] = page_num\n",
    "            return [parsed_json]\n",
    "        elif isinstance(parsed_json, list):\n",
    "            return parsed_json\n",
    "        else:\n",
    "            print(f\"   - ERROR: Unexpected JSON type: {type(parsed_json)}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   - ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In Cell 3\n",
    "def stitch_json_results(all_pages_data):\n",
    "    \"\"\"\n",
    "    Merges the list of JSON objects from each page into a single,\n",
    "    hierarchically correct JSON structure. Now with improved error handling.\n",
    "    \"\"\"\n",
    "    if not all_pages_data: return []\n",
    "    final_data = []\n",
    "    \n",
    "    for i, page_data in enumerate(all_pages_data):\n",
    "        if not page_data: continue\n",
    "\n",
    "        for org_data in page_data:\n",
    "            # --- NEW ROBUSTNESS CHECK ---\n",
    "            # Ensure org_data is a dictionary with the required key\n",
    "            if not isinstance(org_data, dict) or 'organization_name_english' not in org_data:\n",
    "                print(f\"  - WARNING: Skipping malformed organization data on page {i+1}. Data was: {org_data}\")\n",
    "                continue\n",
    "            # --- END ROBUSTNESS CHECK ---\n",
    "\n",
    "            # Check if this organization is a continuation of the previous one\n",
    "            if (final_data and final_data[-1].get('organization_name_english') == org_data.get('organization_name_english')):\n",
    "                last_org = final_data[-1]\n",
    "                if org_data.get('positions'):\n",
    "                    if (last_org.get('positions') and org_data.get('positions') and last_org['positions'][-1].get('title_english') == org_data['positions'][0].get('title_english')):\n",
    "                        last_org['positions'][-1]['personnel'].extend(org_data['positions'][0].get('personnel', []))\n",
    "                        last_org['positions'].extend(org_data['positions'][1:])\n",
    "                    else:\n",
    "                        last_org['positions'].extend(org_data['positions'])\n",
    "                if org_data.get('sub_organizations'):\n",
    "                    last_org['sub_organizations'].extend(org_data.get('sub_organizations', []))\n",
    "            else:\n",
    "                final_data.append(org_data)\n",
    "                \n",
    "    return final_data\n",
    "\n",
    "# In Cell 3, after the stitch_json_results function\n",
    "def save_data_to_file(data, filepath):\n",
    "    \"\"\"Saves the provided data structure to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"  - CRITICAL WARNING: Failed to save data to {filepath}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caa68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 4 (Execution)\n",
    "import time\n",
    "\n",
    "print(f\"Starting processing for '{PDF_PATH}'...\")\n",
    "print(f\"Using model: '{MODEL_NAME}' at host '{OLLAMA_HOST}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- RESUME LOGIC: Load existing data if it exists ---\n",
    "all_pages_data = []\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    print(f\"Found existing output file at '{OUTPUT_PATH}'. Attempting to load and resume.\")\n",
    "    try:\n",
    "        with open(OUTPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "            print(\"Starting a fresh run, but will save progress incrementally.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - WARNING: Could not read existing file. Starting fresh. Error: {e}\")\n",
    "\n",
    "all_pages_data = [] # This will hold the JSON from each page\n",
    "start_page = 0      # Default to starting from the beginning\n",
    "\n",
    "# A better resume strategy:\n",
    "# We will create a temporary cache directory to store the result of each page.\n",
    "CACHE_DIR = \"page_cache\"\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "# --- NEW: OLLAMA SERVER PRE-CHECK ---\n",
    "print(f\"Attempting to connect to Ollama server at {OLLAMA_HOST}...\")\n",
    "try:\n",
    "    client = ollama.Client(host=OLLAMA_HOST, timeout=10) # Short timeout for pre-check\n",
    "    client.list() # A lightweight command to check connectivity\n",
    "    print(\"  -> Successfully connected to Ollama server.\")\n",
    "except Exception as e:\n",
    "    print(f\"  - FATAL ERROR: Could not connect to Ollama server at {OLLAMA_HOST}.\")\n",
    "    print(f\"  - Error details: {e}\")\n",
    "    print(\"  - Please ensure Ollama is running and accessible.\")\n",
    "    ollama_available = False\n",
    "else:\n",
    "    ollama_available = True\n",
    "\n",
    "\n",
    "# In Cell 4 (Execution) - replace the pages_to_run calculation section\n",
    "if ollama_available:\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"FATAL ERROR: PDF file not found at '{PDF_PATH}'\")\n",
    "    else:\n",
    "        doc = fitz.open(PDF_PATH)\n",
    "        num_pages = len(doc)\n",
    "        \n",
    "        # --- NEW: Page Range Logic ---\n",
    "        if START_PAGE is None and END_PAGE is None:\n",
    "            # Process entire document\n",
    "            start_page_idx = 0\n",
    "            end_page_idx = num_pages\n",
    "            print(f\"Processing entire document: {num_pages} pages\")\n",
    "        else:\n",
    "            # Process specific range\n",
    "            start_page_idx = (START_PAGE - 1) if START_PAGE is not None else 0\n",
    "            end_page_idx = END_PAGE if END_PAGE is not None else num_pages\n",
    "            \n",
    "            # Validate page range\n",
    "            if start_page_idx < 0:\n",
    "                start_page_idx = 0\n",
    "            if end_page_idx > num_pages:\n",
    "                end_page_idx = num_pages\n",
    "            if start_page_idx >= end_page_idx:\n",
    "                print(f\"ERROR: Invalid page range. START_PAGE ({START_PAGE}) must be less than END_PAGE ({END_PAGE})\")\n",
    "                print(f\"Document has {num_pages} pages.\")\n",
    "                start_page_idx = 0\n",
    "                end_page_idx = 0\n",
    "            \n",
    "            pages_to_process = end_page_idx - start_page_idx\n",
    "            print(f\"Processing pages {start_page_idx + 1} to {end_page_idx} ({pages_to_process} pages) out of {num_pages} total pages\")\n",
    "        \n",
    "        if start_page_idx >= end_page_idx:\n",
    "            print(\"No pages to process. Exiting.\")\n",
    "        else:\n",
    "            total_start_time = time.time()\n",
    "            successful_pages_count = 0\n",
    "            attempted_pages_count = 0\n",
    "            \n",
    "            # --- MAIN LOOP WITH INCREMENTAL SAVING ---\n",
    "            for i in range(start_page_idx, end_page_idx):\n",
    "                page_num = i + 1\n",
    "                page_cache_path = os.path.join(CACHE_DIR, f\"page_{page_num}.json\")\n",
    "                page_successfully_processed_or_cached = False # Flag for this page\n",
    "                \n",
    "                print(f\"\\n--- Processing Page {page_num} of {num_pages} (batch: {i - start_page_idx + 1}/{end_page_idx - start_page_idx}) ---\")\n",
    "\n",
    "                # --- RESUME LOGIC: Check if page is already cached ---\n",
    "                if os.path.exists(page_cache_path):\n",
    "                    print(f\"  - Result: Page {page_num} already processed. Loading from cache.\")\n",
    "                    try:\n",
    "                        with open(page_cache_path, 'r', encoding='utf-8') as f:\n",
    "                            cached_data = json.load(f)\n",
    "                        if cached_data is None: # Indicates a previous failure\n",
    "                            print(\"  - Cached file indicates a previous failure. Re-processing.\")\n",
    "                            # Do not append to all_pages_data yet, proceed to reprocessing\n",
    "                        else:\n",
    "                            print(\"  - Successfully loaded valid data from cache.\")\n",
    "                            all_pages_data.append(cached_data) # Add valid cached data\n",
    "                            successful_pages_count += 1\n",
    "                            page_successfully_processed_or_cached = True\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(\"  - WARNING: Cache file is corrupted. Re-processing.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  - WARNING: Could not load from cache ({e}). Re-processing.\")\n",
    "\n",
    "                if page_successfully_processed_or_cached:\n",
    "                    continue # Skip to the next page\n",
    "                \n",
    "                # --- If not cached, or cache indicated failure, process the page normally ---\n",
    "                page_start_time = time.time()\n",
    "                page = doc[i]\n",
    "                \n",
    "                print(\"  - Step 1: Pre-processing page text...\")\n",
    "                page_text = preprocess_pdf_page(page)\n",
    "                \n",
    "                if not page_text.strip():\n",
    "                    print(\"  - Result: Page is empty, skipping.\")\n",
    "                    # Cache the empty result so we don't re-process it\n",
    "                    with open(page_cache_path, 'w', encoding='utf-8') as f: json.dump(None, f)\n",
    "                    continue\n",
    "                \n",
    "                print(\"  - Step 2: Calling LLM for JSON extraction...\")\n",
    "                print(\"--- DEBUG: Preprocessed Page Text START ---\")\n",
    "                print(page_text)\n",
    "                print(\"--- DEBUG: Preprocessed Page Text END ---\")\n",
    "                print(\"  - Step 2: Calling LLM for JSON extraction...\")\n",
    "                page_json = get_json_from_llm(page_text, MODEL_NAME, OLLAMA_HOST, page_num)  # Add page_num parameter\n",
    "                \n",
    "                # Cache the result, whether it's successful (JSON) or a failure (None)\n",
    "                with open(page_cache_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(page_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "                if page_json:\n",
    "                    all_pages_data.append(page_json)\n",
    "                    successful_pages_count += 1\n",
    "                    num_orgs = len(page_json)\n",
    "                    print(f\"  - Step 3: Success! Extracted {num_orgs} top-level organization(s) from page.\")\n",
    "                    if DEBUG_PRINT_JSON:\n",
    "                        print(\"    -- Debug: JSON output for this page --\"); print(json.dumps(page_json, ensure_ascii=False, indent=2)); print(\"    -- End Debug --\")\n",
    "                else:\n",
    "                    print(\"  - Step 3: Failed to extract valid JSON from this page. Skipping.\")\n",
    "                \n",
    "                page_end_time = time.time()\n",
    "                page_duration = page_end_time - page_start_time\n",
    "                print(f\"--- Page {page_num} finished in {page_duration:.2f} seconds ---\")\n",
    "\n",
    "            # --- FINAL STITCHING AND SAVING ---\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"All pages processed or loaded from cache. Now stitching final results...\")\n",
    "            \n",
    "            final_stitched_data = stitch_json_results(all_pages_data)\n",
    "            print(\"Stitching complete.\")\n",
    "\n",
    "            print(f\"Writing final structured data to '{OUTPUT_PATH}'...\")\n",
    "            save_data_to_file(final_stitched_data, OUTPUT_PATH)\n",
    "\n",
    "            total_end_time = time.time()\n",
    "            total_duration = total_end_time - total_start_time\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"✅ PROCESSING COMPLETE ✅\")\n",
    "            print(f\"Processed pages {start_page_idx + 1} to {end_page_idx} ({end_page_idx - start_page_idx} pages)\")\n",
    "            print(f\"Successfully processed and extracted data from {successful_pages_count} page(s).\")\n",
    "            print(f\"Total execution time: {total_duration:.2f} seconds.\")\n",
    "            print(f\"Final output saved to: {OUTPUT_PATH}\")\n",
    "            if successful_pages_count == 0 and (end_page_idx - start_page_idx) > 0:\n",
    "                print(f\"WARNING: No data was successfully extracted. {OUTPUT_PATH} will contain an empty list.\")\n",
    "\n",
    "\n",
    "elif not ollama_available:\n",
    "    print(\"Skipped PDF processing as Ollama server was not available.\")\n",
    "else: # Ollama available, but PDF might have been missing or other setup issue\n",
    "    print(\"Skipped PDF processing due to missing PDF file or other setup issue prior to page loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debugging cell to check the cache contents\n",
    "import json\n",
    "import os\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "print(\"=== CACHE DIRECTORY CONTENTS ===\")\n",
    "\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    cache_files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.json')]\n",
    "    print(f\"Found {len(cache_files)} cache files: {cache_files}\")\n",
    "    \n",
    "    for cache_file in sorted(cache_files):\n",
    "        cache_path = os.path.join(CACHE_DIR, cache_file)\n",
    "        print(f\"\\n--- {cache_file} ---\")\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if data is None:\n",
    "                print(\"  Content: None (indicates processing failure)\")\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"  Content: List with {len(data)} items\")\n",
    "                if data:\n",
    "                    print(f\"  First item keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not a dict'}\")\n",
    "            else:\n",
    "                print(f\"  Content: {type(data)} - {str(data)[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading file: {e}\")\n",
    "else:\n",
    "    print(\"Cache directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a183ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this enhanced debugging cell to see the actual content\n",
    "import json\n",
    "import os\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "print(\"=== DETAILED CACHE ANALYSIS ===\")\n",
    "\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    cache_files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.json')]\n",
    "    print(f\"Found {len(cache_files)} cache files: {cache_files}\")\n",
    "    \n",
    "    for cache_file in sorted(cache_files):\n",
    "        cache_path = os.path.join(CACHE_DIR, cache_file)\n",
    "        print(f\"\\n--- {cache_file} ---\")\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if data is None:\n",
    "                print(\"  Content: None (indicates processing failure)\")\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"  Content: List with {len(data)} items\")\n",
    "                for i, item in enumerate(data):\n",
    "                    print(f\"  Item {i+1}: {type(item)}\")\n",
    "                    if isinstance(item, dict):\n",
    "                        print(f\"    Keys: {list(item.keys())}\")\n",
    "                        print(f\"    Full content: {json.dumps(item, ensure_ascii=False, indent=4)}\")\n",
    "                    else:\n",
    "                        print(f\"    Content: {item}\")\n",
    "            else:\n",
    "                print(f\"  Content: {type(data)} - {str(data)[:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading file: {e}\")\n",
    "else:\n",
    "    print(\"Cache directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd943e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new system with a known example\n",
    "test_text = \"\"\"\n",
    "EN: Central Commission for Comprehensive Rule of Law | CN: 中央全面依法治国委員会(2018. 3) | DATE: \n",
    "EN: Chairperson | CN: 主任 | DATE: \n",
    "EN: Xi Jinping | CN: ☆習近平(53.6) | DATE: 18. 8\n",
    "EN: Deputy Directors | CN: 副主任 | DATE: \n",
    "EN: He Rong (f) | CN: ◎賀 栄(女 62.10) | DATE: 23. 2\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== TESTING NEW SYSTEM ===\")\n",
    "result = get_json_from_llm(test_text, MODEL_NAME, OLLAMA_HOST, 32)\n",
    "if result:\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce40ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared\n",
      "Output file cleared\n"
     ]
    }
   ],
   "source": [
    "# Clear cache cell\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(\"Cache cleared\")\n",
    "    \n",
    "# Also clear output file\n",
    "if os.path.exists(\"output.json\"):\n",
    "    os.remove(\"output.json\")\n",
    "    print(\"Output file cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "china_dir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
